{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium"
      ],
      "metadata": {
        "id": "8tisui_KMOrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[atari]"
      ],
      "metadata": {
        "id": "yw0sEil4MGjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[accept-rom-license]"
      ],
      "metadata": {
        "id": "VynUr7mALGW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from random import sample\n",
        "from torch.distributions import Normal\n",
        "\n",
        "class DQN(torch.nn.Module):\n",
        "    def __init__(self, taille_state, taille_action, lr = 3e-4, hidden = 24):\n",
        "        super().__init__()\n",
        "        self.taille_state = taille_state\n",
        "        self.taille_action = taille_action\n",
        "        self.lr = lr\n",
        "        self.hidden = hidden\n",
        "        self.net = torch.nn.Sequential(\n",
        "                        torch.nn.Linear(self.taille_state, self.hidden),\n",
        "                        torch.nn.ReLU(),\n",
        "                        torch.nn.Linear(self.hidden, self.hidden),\n",
        "                        torch.nn.ReLU(),\n",
        "                        torch.nn.Linear(self.hidden, self.taille_action)\n",
        "        )\n",
        "        self.optim = torch.optim.Adam(self.parameters(), lr = self.lr)\n",
        "        self.f_loss = torch.nn.MSELoss()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "    \n",
        "    def fit(self, x, y, epoch=10):\n",
        "        for _ in range(epoch):\n",
        "            y_hat = self.forward(x)\n",
        "            loss = self.f_loss(y_hat, y)\n",
        "            loss.backward()\n",
        "            self.optim.step()\n",
        "            self.optim.zero_grad()\n",
        "\n",
        "    def updateParam(self, dqn):\n",
        "        \"\"\"\n",
        "            Pour Target Network, copier les parametres d'autre DQN\n",
        "        \"\"\"\n",
        "        self.load_state_dict(dqn.state_dict())\n",
        "\n",
        "    def getNet(self):\n",
        "        return self.net\n",
        "\n",
        "class Critic(torch.nn.Module):\n",
        "    def __init__(self, taille_state, taille_action, hidden):\n",
        "        super().__init__()\n",
        "        self.taille_state = taille_state\n",
        "        self.taille_action = taille_action\n",
        "        self.hidden = hidden\n",
        "        self.net = torch.nn.Sequential(\n",
        "                        torch.nn.Linear(self.taille_state, self.hidden[0]),\n",
        "                        torch.nn.ReLU(),\n",
        "                        torch.nn.Linear(self.hidden[0], self.hidden[1]),\n",
        "                        torch.nn.ReLU(),\n",
        "                        torch.nn.Linear(self.hidden[1], self.taille_action)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "    \n",
        "    def getNet(self):\n",
        "        return self.net\n",
        "\n",
        "\n",
        "class Actor(torch.nn.Module):\n",
        "    def __init__(self, taille_state, taille_action, hidden, out=0):\n",
        "        super().__init__()\n",
        "        self.taille_state = taille_state\n",
        "        self.taille_action = taille_action\n",
        "        self.hidden = hidden\n",
        "        if out == 0:\n",
        "            self.net = torch.nn.Sequential(\n",
        "                            torch.nn.Linear(self.taille_state, self.hidden[0]),\n",
        "                            torch.nn.ReLU(),\n",
        "                            torch.nn.Linear(self.hidden[0], self.hidden[1]),\n",
        "                            torch.nn.ReLU(),\n",
        "                            torch.nn.Linear(self.hidden[1], self.taille_action),\n",
        "                            torch.nn.Tanh() # lunar action [-1,1]\n",
        "            )\n",
        "        else:\n",
        "            self.net = torch.nn.Sequential(\n",
        "                            torch.nn.Linear(self.taille_state, self.hidden[0]),\n",
        "                            torch.nn.ReLU(),\n",
        "                            torch.nn.Linear(self.hidden[0], self.hidden[1]),\n",
        "                            torch.nn.ReLU(),\n",
        "                            torch.nn.Linear(self.hidden[1], self.taille_action),\n",
        "                            torch.nn.Softmax()\n",
        "            )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "    def getNet(self):\n",
        "        return self.net\n",
        "\n",
        "\n",
        "\n",
        "# Just to test Optimal Actor, forgot to change name when training ...\n",
        "# class NNActor(torch.nn.Module):\n",
        "#     def __init__(self, taille_state, taille_action, lr = 0.00025, hidden = 256):\n",
        "#         super().__init__()\n",
        "#         self.taille_state = taille_state\n",
        "#         self.taille_action = taille_action\n",
        "#         self.lr = lr\n",
        "#         self.hidden = hidden\n",
        "#         self.net = torch.nn.Sequential(\n",
        "#                         torch.nn.Linear(self.taille_state, 256),\n",
        "#                         torch.nn.ReLU(),\n",
        "#                         torch.nn.Linear(256, 256),\n",
        "#                         torch.nn.ReLU(),\n",
        "#                         torch.nn.Linear(256, self.taille_action),\n",
        "#                         torch.nn.Tanh() # lunar action [-1,1]\n",
        "#         )\n",
        "#         self.optim = torch.optim.Adam(self.parameters(), lr = self.lr)\n",
        "#         self.f_loss = torch.nn.MSELoss()\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.net(x)\n",
        "\n",
        "#     def getNet(self):\n",
        "#         return self.net\n",
        "\n",
        "class Buffer:\n",
        "    def __init__(self, taille_max):\n",
        "        self.taille = taille_max\n",
        "        self.memoire = []\n",
        "        self.last100 = []\n",
        "        self.mean = []\n",
        "        \n",
        "    def getLen(self):\n",
        "        return len(self.memoire)\n",
        "      \n",
        "    def getLastMean(self):\n",
        "        return self.mean[-1]\n",
        "    \n",
        "    def add(self, element):\n",
        "        if self.getLen() >= self.taille:\n",
        "            del self.memoire[0]\n",
        "        self.memoire.append(element)\n",
        "\n",
        "    def sampleState(self, taille_sample):\n",
        "        return sample(self.memoire, taille_sample)\n",
        "\n",
        "    def addReward(self, reward):\n",
        "        if len(self.last100) >= 100:\n",
        "            del self.last100[0]\n",
        "        self.last100.append(reward)\n",
        "        self.mean.append(sum(self.last100)/len(self.last100))\n",
        "    \n",
        "def addGaussianNoise(action, sigma=0.1):\n",
        "    a = torch.tensor(action)\n",
        "    dist = Normal(a, sigma)\n",
        "    return dist.sample().detach().numpy()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wlMYQ0iEMsys"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfgDDGP = {\n",
        "    \"len_buffer\" : 100000,\n",
        "    \"batch\" : 64,\n",
        "    \"gamma\" : 0.99,\n",
        "    \"tau\" : 0.001,\n",
        "    \"explore\": 1.0,\n",
        "    \"explore_min\": 0.01,\n",
        "    \"explore_decay\": 0.995,\n",
        "    \"noise\": 0.1,\n",
        "    \"noise_decay_step\": 5000,\n",
        "    \"start_learning_step\": 10000,\n",
        "    \"lr\" : {\n",
        "        \"critic\" : 0.0002,\n",
        "        \"actor\" : 0.0003\n",
        "    },\n",
        "    \"hidden_layer\" : {\n",
        "        \"critic\" : [256,256],\n",
        "        \"actor\" : [256, 256]\n",
        "    }\n",
        "}\n",
        "\n",
        "cfgA2C1step = {\n",
        "    \"entropy\": False,\n",
        "    \"gamma\" : 0.99,\n",
        "    \"beta\" :0.9,\n",
        "    \"lr\" : {\n",
        "        \"critic\" : 1e-3,\n",
        "        \"actor\" : 1e-3\n",
        "    },\n",
        "    \"hidden_layer\" : {\n",
        "        \"critic\" : [64,32],\n",
        "        \"actor\" : [64, 32]\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "-y1oOCRcMwH7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68a4c851-5c28-4ee8-ba8a-1a1e1ee67b4c"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import e\n",
        "import os\n",
        "import sys\n",
        "import copy as cp\n",
        "from torch.distributions.uniform import Uniform\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "sys.path.append(os.path.abspath(\"../Network\"))\n",
        "\n",
        "class Agent():\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "    \n",
        "    def act(self, obs):\n",
        "        pass\n",
        "\n",
        "    def store(self, obs, action, new_obs, reward):\n",
        "        pass\n",
        "\n",
        "class AgentRandom(Agent):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "\n",
        "    def act(self, obs):\n",
        "        return self.env.action_space.sample()\n",
        "\n",
        "    def store(self, obs, action, new_obs, reward):\n",
        "        pass\n",
        "\n",
        "class AgentPolicy(Agent):\n",
        "    def __init__(self, env, pi):\n",
        "        super().__init__(env)\n",
        "        self.pi = pi\n",
        "\n",
        "    def act(self, obs):\n",
        "        return self.pi[obs]\n",
        "\n",
        "    def store(self, obs, action, new_obs, reward):\n",
        "        pass\n",
        "\n",
        "class AgentDQN(Agent):\n",
        "    def __init__(self,env, gamma=0.99, batch=32):\n",
        "        super().__init__(env)\n",
        "        self.taille_state = self.env.observation_space.shape[0] #Box\n",
        "        self.taille_action = self.env.action_space.n\n",
        "        self.batch_size = batch\n",
        "        self.buffer = Buffer(5*self.batch_size)\n",
        "        # NN\n",
        "        self.dqn = DQN(self.taille_state, self.taille_action)\n",
        "        # Hyper-param\n",
        "        self.explore = 1.0\n",
        "        self.explore_min = 0.01\n",
        "        self.explore_decay = 0.995\n",
        "        self.gamma = gamma\n",
        "        \n",
        "    def act(self, state):\n",
        "        if torch.rand(1).item() < self.explore: # solf greedy\n",
        "            return torch.randint(0, self.taille_action, (1,)).item()\n",
        "        return torch.argmax(self.dqn(state)).item()\n",
        "    \n",
        "    def act_opt(self, state):\n",
        "        return torch.argmax(self.dqn(state)).item()\n",
        "    \n",
        "    def setDQN(self, path):\n",
        "        self.dqn = torch.load(path)\n",
        "\n",
        "    def store(self, state, reward, action, done,  state_suivant):\n",
        "        self.buffer.add([state, reward, action, done, state_suivant])\n",
        "\n",
        "    def replay(self, batch_seuil, decay):\n",
        "        if self.buffer.getLen() < batch_seuil:\n",
        "            return\n",
        "        mini_batch = self.buffer.sampleState(self.batch_size)\n",
        "        for state, reward, action, done, state_suivant in mini_batch:\n",
        "            y_action = reward + self.gamma*torch.max(self.dqn.forward(state_suivant)).detach().item()\n",
        "            if done:\n",
        "                y_action = reward # bah, si done -> perdu donc faut savoir pour eviter\n",
        "            y = self.dqn.forward(state)\n",
        "            y[0][action] = y_action # tel action amene a tel score\n",
        "            self.dqn.fit(state, y, epoch=1) \n",
        "        if decay and self.explore > self.explore_min:\n",
        "            self.explore *= self.explore_decay\n",
        "\n",
        "class AgentDQN_TargetNetwork(AgentDQN):\n",
        "    def __init__(self, env, gamma=0.99, batch=32, K=32):\n",
        "        super().__init__(env, gamma, batch)\n",
        "        self.K = K # nombre de pas pour maj Target Network\n",
        "        self.counterK = 0 \n",
        "        self.dqnTarget = cp.deepcopy(self.dqn)\n",
        "\n",
        "    def replay(self, batch_seuil, decay):\n",
        "        if self.buffer.getLen() < batch_seuil:\n",
        "            return\n",
        "        mini_batch = self.buffer.sampleState(self.batch_size)\n",
        "        for state, reward, action, done, state_suivant in mini_batch:\n",
        "            # target est calculé par Target Network\n",
        "            y_action = reward + self.gamma*torch.max(self.dqnTarget.forward(state_suivant)).detach().item()\n",
        "            if done:\n",
        "                y_action = reward # bah, si done -> no more futur\n",
        "            y = self.dqnTarget.forward(state)\n",
        "            y[0][action] = y_action # tel action amene a tel score, 1 batch, tensor donc [0] ..\n",
        "            # MAJ Q-network\n",
        "            self.dqn.fit(state, y, epoch=1)\n",
        "            self.counterK += 1\n",
        "            # Update target network every K steps ...\n",
        "            if self.counterK == self.K:\n",
        "                self.dqnTarget.updateParam(self.dqn)\n",
        "                self.counterK = 0 # reset counter, ugly code ...\n",
        "        if decay and self.explore > self.explore_min:\n",
        "            self.explore *= self.explore_decay\n",
        "\n",
        "\n",
        "\n",
        "class AgentDDPG(Agent):\n",
        "    def __init__(self, env, config):\n",
        "        super().__init__(env)\n",
        "        # continuous environment\n",
        "        assert env.continuous == True\n",
        "        # param env\n",
        "        if env.unwrapped.spec.id == \"LunarLander-v2\":\n",
        "            self.taille_state = self.env.observation_space.shape[0] #Box\n",
        "            self.taille_action = self.env.action_space.shape[0] #Box\n",
        "        # Neural networks\n",
        "        self.Critic = Critic(self.taille_state + self.taille_action, 1, config[\"hidden_layer\"][\"critic\"])\n",
        "        self.CriticTarget = cp.deepcopy(self.Critic)\n",
        "        self.Actor = Actor(self.taille_state , self.taille_action, config[\"hidden_layer\"][\"actor\"]) # I make only 1 actor here\n",
        "        self.ActorTarget = cp.deepcopy(self.Actor)\n",
        "        # Optim\n",
        "        self.optCritic = torch.optim.Adam(self.Critic.parameters(), config[\"lr\"][\"critic\"]) # optim Critic\n",
        "        self.optActor = torch.optim.Adam(self.Actor.parameters(), config[\"lr\"][\"actor\"]) # optim Actor\n",
        "   \n",
        "        # Loss\n",
        "        self.f_loss = torch.nn.MSELoss()\n",
        "        # Hyper-param\n",
        "        self.batch_size = config[\"batch\"]\n",
        "        self.buffer = Buffer(config[\"len_buffer\"])\n",
        "        self.explore = config[\"explore\"]\n",
        "        self.explore_min = config[\"explore_min\"]\n",
        "        self.explore_decay = config[\"explore_decay\"]\n",
        "        self.gamma = config[\"gamma\"]\n",
        "        self.tau = config[\"tau\"] # Tau to update Target Network\n",
        "        self.sigma = config[\"noise\"] # noise\n",
        "        self.decay_sigma = config[\"noise_decay_step\"]\n",
        "        self.step_count = 0\n",
        "        self.start_learning = config[\"start_learning_step\"]\n",
        "\n",
        "    def act(self, state):\n",
        "        self.step_count += 1\n",
        "        if torch.rand(1).item() < self.explore: # solf greedy\n",
        "            if self.env.unwrapped.spec.id == \"LunarLander-v2\":\n",
        "                return np.random.uniform(-1,1,(2,)) # np.array([main, lateral])\n",
        "        if self.step_count>0 and self.step_count%self.decay_sigma==0: #decay noise every 5000 steps\n",
        "            self.sigma *= 0.95\n",
        "        a = self.Actor(state)[0].cpu().detach().numpy()\n",
        "        a = addGaussianNoise(a, self.sigma)\n",
        "        return a\n",
        "    \n",
        "    def act_opt(self, state): # 1 actor\n",
        "        return self.Actor(state)[0].detach().numpy()\n",
        "    \n",
        "    def saveActor(self, path):\n",
        "        torch.save(self.Actor, path)\n",
        "    \n",
        "    def setActor(self, path): \n",
        "        self.Actor = torch.load(path)\n",
        "\n",
        "    def addReward(self, reward):\n",
        "        \"\"\"\n",
        "            Tracking last N-reward\n",
        "        \"\"\"\n",
        "        self.buffer.addReward(reward)\n",
        "        return self.buffer.getLastMean()\n",
        "\n",
        "    def store(self, state, reward, action, done,  state_suivant):\n",
        "        self.buffer.add([state, reward, action, done, state_suivant])\n",
        "\n",
        "    def updateTargetDDPG(self):\n",
        "        # critic\n",
        "        for param, target_param in zip(self.Critic.parameters(), self.CriticTarget.parameters()):\n",
        "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "        # actor\n",
        "        for param, target_param in zip(self.Actor.parameters(), self.ActorTarget.parameters()):\n",
        "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "    def updateNetworks(self, state, reward, action, done, state_suivant):\n",
        "        assert list(state.shape) == [1, self.taille_state]\n",
        "        assert list(action.shape) == [1, self.taille_action]\n",
        "        state_suivant.requires_grad = True # to update Actor\n",
        "        # prepare data actor\n",
        "        outActor_suivant = self.ActorTarget.forward(state_suivant) # pi(state_suivant)\n",
        "        # prepare data critic\n",
        "        input_Q = torch.hstack((state, action))\n",
        "        input_Q_suivant = torch.hstack((state_suivant, outActor_suivant)) \n",
        "        #\n",
        "        Q = self.Critic.forward(input_Q) # y_hat\n",
        "        Q_suivant = self.CriticTarget.forward(input_Q_suivant)\n",
        "        y = reward + self.gamma*Q_suivant\n",
        "        if done:\n",
        "            y = torch.tensor(reward, dtype=torch.float32).view(1,-1)\n",
        "        # Critic, 1 epoch\n",
        "        loss = self.f_loss(Q, y)\n",
        "        self.optCritic.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optCritic.step()\n",
        "\n",
        "    def replay(self, decay):\n",
        "        if self.buffer.getLen() < self.start_learning:\n",
        "            return\n",
        "        mini_batch = self.buffer.sampleState(self.batch_size)\n",
        "        batch_state = []\n",
        "        for state, reward, action, done, state_suivant in mini_batch:\n",
        "            batch_state.append(state.squeeze(0))\n",
        "            self.updateNetworks(state, reward, action, done, state_suivant)\n",
        "        batch_state = torch.stack(batch_state)\n",
        "        # Actor\n",
        "        outActor = self.Actor.forward(batch_state)\n",
        "        intputQ_actor_update = torch.hstack((batch_state, outActor))\n",
        "        lossActor = -self.Critic(intputQ_actor_update).mean() # if Q is bad -> loss is positif, Q is good -> loss is neg (good thing)\n",
        "        self.optActor.zero_grad()\n",
        "        lossActor.backward()\n",
        "        self.optActor.step()\n",
        "        # Target\n",
        "        self.updateTargetDDPG()\n",
        "        if decay and self.explore > self.explore_min:\n",
        "            self.explore *= self.explore_decay\n",
        "\n",
        "class AgentA2C_1step(Agent):\n",
        "    def __init__(self, env, config):\n",
        "        super().__init__(env)\n",
        "        # entropy ou pas\n",
        "        self.useEntropy = config[\"entropy\"]\n",
        "        # param env\n",
        "        if env.unwrapped.spec.id == \"CartPole-v1\":\n",
        "            self.taille_state = self.env.observation_space.shape[0] #Box\n",
        "            self.taille_action = self.env.action_space.n #discrete\n",
        "            # Neural networks\n",
        "            self.Critic = Critic(self.taille_state, 1, config[\"hidden_layer\"][\"critic\"])\n",
        "            self.Actor = Actor(self.taille_state , self.taille_action, config[\"hidden_layer\"][\"actor\"], out = 1) # out 1 softmax\n",
        "            self.optCritic = torch.optim.Adam(self.Critic.parameters(), config[\"lr\"][\"critic\"]) # optim Critic\n",
        "            self.optActor = torch.optim.Adam(self.Actor.parameters(), config[\"lr\"][\"actor\"]) # optim Actor\n",
        "            self.lossCritic = torch.nn.MSELoss()\n",
        "        # hyper param\n",
        "        self.gamma = config[\"gamma\"]\n",
        "        self.beta = config[\"beta\"] # entropy\n",
        "\n",
        "    def act(self, state, mode=0):\n",
        "        \"\"\"\n",
        "            mode: 0 stochastics, 1 max\n",
        "        \"\"\"\n",
        "        proba = self.Actor(state)\n",
        "        if mode == 1:\n",
        "            return proba, proba.argmax(1)\n",
        "        return proba, Categorical(proba).sample()\n",
        "    \n",
        "    def updateNetworks(self, state, reward, action, done, state_suivant, dist):\n",
        "        # critic\n",
        "        Q = reward + (1-done)*self.gamma*self.Critic.forward(state_suivant) - self.Critic.forward(state)\n",
        "        V = self.Critic.forward(state)\n",
        "\n",
        "        A = Q - V\n",
        "        # actor\n",
        "\n",
        "        loss = self.lossCritic(V, Q)\n",
        "        self.optCritic.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optCritic.step()\n",
        "\n",
        "        # actor\n",
        "        if self.useEntropy:\n",
        "          loss_actor = -dist.log_prob(action)*(A.detach() - self.beta*dist.entropy().item())\n",
        "        else:\n",
        "          loss_actor = -dist.log_prob(action)*(A.detach())\n",
        "        self.optActor.zero_grad()\n",
        "        loss_actor.backward()\n",
        "        self.optActor.step()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PO0KU9E1MVfa"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import math\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    env = gym.make(\"CartPole-v1\")\n",
        "    nb_episode = 5000\n",
        "    r = []\n",
        "    agent = AgentA2C_1step(env, cfgA2C1step)\n",
        "    for i in tqdm(range(nb_episode)):\n",
        "        state, _ = env.reset()\n",
        "        state = torch.from_numpy(state).float()\n",
        "        cum_reward = 0\n",
        "        for frame in range(100000):\n",
        "            proba, action = agent.act(state)\n",
        "            dist = Categorical(proba)\n",
        "            state_suivant, reward, done, _, info = env.step(action.detach().data.numpy())\n",
        "            state_suivant = torch.from_numpy(state_suivant).float()\n",
        "            #transform action \n",
        "            agent.updateNetworks(state, reward, action, done, state_suivant, dist)\n",
        "            cum_reward += reward\n",
        "            if done:\n",
        "                r.append(cum_reward)\n",
        "                break\n",
        "            state = torch.tensor(state_suivant)\n",
        "\n"
      ],
      "metadata": {
        "id": "rYp5jxr4KwXF",
        "outputId": "6a9b70d5-0a19-49ec-9306-9aec66f19d42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/5000 [00:00<?, ?it/s]<ipython-input-137-064dfee97fa8>:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  state = torch.tensor(state_suivant)\n",
            "100%|██████████| 5000/5000 [06:46<00:00, 12.31it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epi = np.arange(nb_episode)\n",
        "rw = np.array(r)\n",
        "\n",
        "plt.plot(epi,rw)\n",
        "plt.show"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "ZJjVLT8N0guw",
        "outputId": "b931c24e-74d6-454b-a3a5-92b13edf2c3c"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show(close=None, block=None)>"
            ]
          },
          "metadata": {},
          "execution_count": 138
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}